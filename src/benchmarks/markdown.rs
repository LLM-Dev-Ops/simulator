//! Markdown report generation
//!
//! Generates markdown-formatted benchmark reports and summaries.

use chrono::{DateTime, Utc};

use super::BenchmarkResult;

/// Generate a full markdown report from benchmark results
pub fn generate_report(results: &[BenchmarkResult]) -> String {
    let mut report = String::new();

    // Header
    report.push_str("# LLM-Simulator Benchmark Report\n\n");
    report.push_str(&format!(
        "Generated: {}\n\n",
        Utc::now().format("%Y-%m-%d %H:%M:%S UTC")
    ));

    // Summary section
    report.push_str("## Summary\n\n");
    report.push_str(&format!("- **Total Benchmarks**: {}\n", results.len()));

    if let Some(first) = results.first() {
        report.push_str(&format!(
            "- **Run Started**: {}\n",
            first.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
        ));
    }
    if let Some(last) = results.last() {
        report.push_str(&format!(
            "- **Run Completed**: {}\n",
            last.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
        ));
    }

    report.push_str("\n## Benchmark Results\n\n");

    // Results table
    report.push_str("| Target | Status | Key Metrics |\n");
    report.push_str("|--------|--------|-------------|\n");

    for result in results {
        let status = if has_error(&result.metrics) {
            "Failed"
        } else {
            "Passed"
        };
        let key_metrics = extract_key_metrics(&result.metrics);
        report.push_str(&format!(
            "| {} | {} | {} |\n",
            result.target_id, status, key_metrics
        ));
    }

    // Detailed results
    report.push_str("\n## Detailed Results\n\n");

    for result in results {
        report.push_str(&format!("### {}\n\n", result.target_id));
        report.push_str(&format!(
            "**Timestamp**: {}\n\n",
            result.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
        ));

        report.push_str("**Metrics**:\n\n");
        report.push_str("```json\n");
        report.push_str(
            &serde_json::to_string_pretty(&result.metrics).unwrap_or_else(|_| "{}".to_string()),
        );
        report.push_str("\n```\n\n");
    }

    // Footer
    report.push_str("---\n\n");
    report.push_str("*Report generated by LLM-Simulator canonical benchmark system*\n");

    report
}

/// Generate a compact summary markdown
pub fn generate_summary(results: &[BenchmarkResult]) -> String {
    let mut summary = String::new();

    summary.push_str("# Benchmark Summary\n\n");
    summary.push_str(&format!(
        "Last Run: {}\n\n",
        Utc::now().format("%Y-%m-%d %H:%M:%S UTC")
    ));

    let passed = results.iter().filter(|r| !has_error(&r.metrics)).count();
    let failed = results.len() - passed;

    summary.push_str(&format!("## Results: {}/{} Passed\n\n", passed, results.len()));

    if failed > 0 {
        summary.push_str("### Failed Benchmarks\n\n");
        for result in results.iter().filter(|r| has_error(&r.metrics)) {
            summary.push_str(&format!("- {}\n", result.target_id));
        }
        summary.push_str("\n");
    }

    summary.push_str("### Benchmark Targets\n\n");
    for result in results {
        let status_icon = if has_error(&result.metrics) {
            "[ ]"
        } else {
            "[x]"
        };
        let metrics = extract_key_metrics(&result.metrics);
        summary.push_str(&format!("- {} **{}**: {}\n", status_icon, result.target_id, metrics));
    }

    summary
}

/// Generate a single benchmark result as markdown
pub fn result_to_markdown(result: &BenchmarkResult) -> String {
    let mut md = String::new();

    md.push_str(&format!("## {}\n\n", result.target_id));
    md.push_str(&format!(
        "- **Timestamp**: {}\n",
        result.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
    ));

    if let Some(obj) = result.metrics.as_object() {
        for (key, value) in obj {
            md.push_str(&format!("- **{}**: {}\n", format_key(key), format_value(value)));
        }
    }

    md.push_str("\n");
    md
}

/// Format a metrics key for display
fn format_key(key: &str) -> String {
    key.replace('_', " ")
        .split_whitespace()
        .map(|word| {
            let mut chars = word.chars();
            match chars.next() {
                None => String::new(),
                Some(first) => first.to_uppercase().chain(chars).collect(),
            }
        })
        .collect::<Vec<_>>()
        .join(" ")
}

/// Format a metrics value for display
fn format_value(value: &serde_json::Value) -> String {
    match value {
        serde_json::Value::Number(n) => {
            if let Some(f) = n.as_f64() {
                if f.fract() == 0.0 && f.abs() < 1e10 {
                    format!("{:.0}", f)
                } else {
                    format!("{:.3}", f)
                }
            } else {
                n.to_string()
            }
        }
        serde_json::Value::String(s) => s.clone(),
        serde_json::Value::Bool(b) => b.to_string(),
        _ => value.to_string(),
    }
}

/// Check if metrics indicate an error
fn has_error(metrics: &serde_json::Value) -> bool {
    if let Some(obj) = metrics.as_object() {
        if let Some(error) = obj.get("error") {
            return !error.is_null() && error != &serde_json::Value::Bool(false);
        }
        if let Some(success) = obj.get("success") {
            if let Some(b) = success.as_bool() {
                return !b;
            }
        }
    }
    false
}

/// Extract key metrics for summary display
fn extract_key_metrics(metrics: &serde_json::Value) -> String {
    let mut parts = Vec::new();

    if let Some(obj) = metrics.as_object() {
        // Prioritize common metrics
        let priority_keys = [
            "throughput_ops_per_sec",
            "latency_avg_ms",
            "latency_p50_ms",
            "duration_ms",
            "samples",
            "iterations",
        ];

        for key in &priority_keys {
            if let Some(value) = obj.get(*key) {
                parts.push(format!("{}: {}", format_key(key), format_value(value)));
                if parts.len() >= 3 {
                    break;
                }
            }
        }

        // If no priority keys found, take first few
        if parts.is_empty() {
            for (key, value) in obj.iter().take(3) {
                if key != "error" && key != "success" {
                    parts.push(format!("{}: {}", format_key(key), format_value(value)));
                }
            }
        }
    }

    if parts.is_empty() {
        "No metrics".to_string()
    } else {
        parts.join(", ")
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_generate_report() {
        let results = vec![
            BenchmarkResult::new(
                "latency-sampling",
                serde_json::json!({
                    "throughput_ops_per_sec": 50000.0,
                    "latency_avg_ms": 0.02
                }),
            ),
            BenchmarkResult::new(
                "response-generation",
                serde_json::json!({
                    "samples": 1000,
                    "duration_ms": 250.5
                }),
            ),
        ];

        let report = generate_report(&results);
        assert!(report.contains("# LLM-Simulator Benchmark Report"));
        assert!(report.contains("latency-sampling"));
        assert!(report.contains("response-generation"));
    }

    #[test]
    fn test_generate_summary() {
        let results = vec![
            BenchmarkResult::new("test-1", serde_json::json!({"value": 100})),
            BenchmarkResult::new("test-2", serde_json::json!({"error": "failed"})),
        ];

        let summary = generate_summary(&results);
        assert!(summary.contains("1/2 Passed"));
        assert!(summary.contains("Failed Benchmarks"));
    }

    #[test]
    fn test_format_key() {
        assert_eq!(format_key("throughput_ops_per_sec"), "Throughput Ops Per Sec");
        assert_eq!(format_key("latency_avg_ms"), "Latency Avg Ms");
    }

    #[test]
    fn test_has_error() {
        assert!(!has_error(&serde_json::json!({"value": 100})));
        assert!(has_error(&serde_json::json!({"error": "something went wrong"})));
        assert!(has_error(&serde_json::json!({"success": false})));
        assert!(!has_error(&serde_json::json!({"success": true})));
    }
}
